#!/usr/bin/python3
from queue import Queue
import argparse
import collections
import os
import pickle
import re
import sys
import threading
import time
import urllib.parse

try:
    import lockfile
    import requests
    from bs4 import BeautifulSoup
except ImportError as e:
    if e.name is None:
        raise
    print('Please install %s.' % e.name, file=sys.stderr)
    sys.exit(1)

class History(set):
    def __init__(self, path):
        self.path = path

    def __enter__(self):
        self.fh = None
        if self.path and os.path.exists(self.path):
            self.lock = lockfile.LockFile(self.path)
            self.lock.acquire()
            self.fh = open(self.path, 'r+b')
            self.update(pickle.load(self.fh))
            print('Loaded %d URLs from history file' % len(self))
        return self

    def __exit__(self, *args):
        if self.fh:
            self.fh.truncate(0)
            self.fh.seek(0)
            s = self.copy()
            pickle.dump(s, self.fh)
            self.fh.close()
            self.lock.release()
            print('Saved %d URLs to history file' % len(s))

class PseudoDict(collections.defaultdict):
    __getattr__ = dict.__getitem__
    __setattr__ = dict.__setitem__
    def __init__(self, *args, **kwargs):
        super().__init__(lambda: None)
        self.update(*args, **kwargs)

class Worker(object):
    num = 0

    def __init__(self):
        self.num = self.__class__.num
        self.__class__.num += 1

    def run(self, queue, lock):
        while True:
            item = queue.get()
            try:
                status = self.process(item, queue, lock)
            except Exception as ex:
                status = self.on_error(item, ex, queue, lock)
            finally:
                queue.task_done()
            with lock:
                print('[T%d ~%5d queued] %s' % (
                    self.num, queue.qsize(), self.format_status(item, status)))

def run_workers(count, queue, worker_factory):
    lock = threading.Lock()
    for i in range(count):
        worker = worker_factory()
        thread = threading.Thread(target=worker.run, args=[queue, lock])
        thread.daemon = True
        thread.start()
    queue.join()

class Command(object):
    pass

class DownloadCommand(Command):
    @staticmethod
    def decorate_parser(parent_parser, fmt):
        for alias in ['download', 'dl']:
            parser = parent_parser.add_parser(
                alias, help='download files', formatter_class=fmt)

            parser.add_argument(
                '-a', '--accept', metavar='REGEX', default='.*',
                help='set regex indicating which URLs to crawl')

            parser.add_argument(
                '-H', '--history', metavar='FILE',
                help='set path to the history file')

            parser.add_argument(
                '-t', '--target', metavar='DIR', default='.',
                help='set base target directory')

            parser.add_argument(
                '-r', '--retries', metavar='NUM', type=int, default=3,
                help='set retry count for failed downloads')

            parser.add_argument(
                '--retry_wait', metavar='NUM', type=int, default=3,
                help='set retry count for failed downloads')

            parser.add_argument(
                '-n', '--num', dest='workers', metavar='NUMBER', type=int,
                default=1, help='set worker count')

            parser.add_argument(
                'url', metavar='URL', nargs='+',
                help='initial URLs to retrieve')

            parser.set_defaults(command=__class__)

    def run(self, args):
        args.accept = re.compile(args.accept)
        visited = set()
        with History(args.history) as history:
            queue = Queue()
            for url in args.url:
                visited.add(url)
                queue.put(DownloadCommand.Item(url))
            f = lambda: __class__.Worker(args, visited, history)
            run_workers(args.workers, queue, f)

    class Worker(Worker):
        def __init__(self, args, visited, history):
            self.args = args
            self.visited = visited
            self.history = history

        def process_html(self, response, queue):
            soup = BeautifulSoup(response.text)
            child_urls = set()
            for link in soup.find_all('a', href=True):
                child_url = urllib.parse.urldefrag(
                    urllib.parse.urljoin(response.url, link['href'])).url
                if self.args.accept.search(child_url) \
                and not child_url in self.visited \
                and not child_url in self.history:
                    child_urls.add(child_url)
                    self.visited.add(child_url)
            for child_url in sorted(child_urls):
                queue.put(DownloadCommand.Item(child_url))
                self.visited.add(child_url)
            return PseudoDict(parsed=True, urls_added=len(child_urls))

        def process_nonhtml(self, response):
            parsed_url = urllib.parse.urlparse(response.url)
            target_path = os.path.join(
                self.args.target,
                parsed_url.netloc,
                re.sub(r'^[\/]*', '', parsed_url.path))
            target_dir = os.path.dirname(target_path)
            os.makedirs(target_dir, exist_ok=True)
            with open(target_path, 'wb') as fh:
                fh.write(response.content)
            self.history.add(response.url)
            return PseudoDict(download_path=target_path)

        def process(self, item, queue, lock):
            response = requests.get(item.url)
            with lock:
                if response.status_code != 200:
                    raise RuntimeError('HTTP error %d' % response.status_code)
                mime = response.headers['content-type'].split(';')[0].lower()
                if mime == 'text/html':
                    return self.process_html(response, queue)
                else:
                    return self.process_nonhtml(response)

        def on_error(self, item, ex, queue, lock):
            time.sleep(self.args.retry_wait)
            item.retries += 1
            if item.retries < self.args.retries:
                queue.put(item)
            return PseudoDict(error=str(ex))

        def format_status(self, item, status):
            messages = []
            if status.error:
                messages.append('error: %s' % status.error)
            if status.download_path:
                messages.append('saved to %s' % status.download_path)
            if status.parsed:
                messages.append('added %d URLs' % status.urls_added)
            if item.retries > 1:
                messages.append('retry #%d' % item.retries)
            return '%s: %s' % (item.url, '; '.join(messages))

    class Item(PseudoDict):
        def __init__(self, url):
            super().__init__(url=url, retries=0)

class PruneCommand(Command):
    @staticmethod
    def decorate_parser(parent_parser, fmt):
        parser = parent_parser.add_parser(
            'prune', help='prune old entries in history file',
            formatter_class=fmt)

        parser.add_argument(
            '-H', '--history', metavar='FILE',
            help='set path to the history file')

        parser.add_argument(
            '-n', '--num', dest='workers', metavar='NUMBER', type=int,
            default=1, help='set worker count')

        parser.set_defaults(command=__class__)

    def run(self, args):
        with History(args.history) as history:
            queue = Queue()
            for url in history:
                queue.put(url)
            f = lambda: __class__.Worker(history)
            run_workers(args.workers, queue, f)

    class Worker(Worker):
        def __init__(self, history):
            self.history = history

        def process(self, item, queue, lock):
            response = requests.head(item.strip())
            with lock:
                if response.status_code == 404:
                    self.history.remove(item)
                    return 'pruned (404)'
                else:
                    return 'spared (%d)' % response.status_code

        def on_error(self, item, ex, queue, lock):
            return 'error: %s' % str(ex)

        def format_status(self, item, status):
            return '%s: %s' % (item.strip(), status)

def parse_args():
    fmt = lambda prog: argparse.HelpFormatter(
        prog, max_help_position=40, width=80)

    parser = argparse.ArgumentParser(
        description='Download images from the Internet', formatter_class=fmt)
    subparsers = parser.add_subparsers(help='choose the command')
    DownloadCommand.decorate_parser(subparsers, fmt)
    PruneCommand.decorate_parser(subparsers, fmt)
    return parser.parse_args()

def main():
    args = parse_args()
    command = args.command()
    command.run(args)

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print('Exiting due to user abort')
