#!/usr/bin/python3
from queue import Queue
import argparse
import os
import pickle
import re
import sys
import threading
import time
import urllib.parse

try:
    import requests
except ImportError:
    print('Please install requests.', file=sys.stderr)
    exit(1)

try:
    from bs4 import BeautifulSoup
except ImportError:
    print('Please install BeautifulSoup4.', file=sys.stderr)
    exit(1)

def read_history_file(path):
    downloaded = set()
    if path and os.path.exists(path):
        with open(path, 'rb') as fh:
            downloaded = pickle.load(fh)
        print('Loaded %d URLs from history file' % len(downloaded))
    return downloaded

def save_history_file(downloaded, path):
    if path:
        with open(path, 'wb') as fh:
            pickle.dump(downloaded, fh)
        print('Saved %d URLs to history file' % len(downloaded))

class Worker(object):
    number = 0

    def __init__(self):
        self.number = self.__class__.number
        self.__class__.number += 1

    def run(self, queue, lock):
        while True:
            item = queue.get()

            try:
                status = self.process(item, queue, lock)
            except Exception as ex:
                status = self.on_error(item, ex, queue, lock)
            finally:
                queue.task_done()

            with lock:
                print('[T%d ~%5d queued] %s' % (
                    self.number,
                    queue.qsize(),
                    self.format_status(item, status)))

    def process(self, item, queue, lock):
        raise NotImplementedError('Override me')

    def on_error(self, item, queue, lock):
        raise NotImplementedError('Override me')

    def format_status(self, item, status):
        raise NotImplementedError('Override me')

def run_workers(count, queue, worker_factory):
    lock = threading.Lock()
    for i in range(count):
        worker = worker_factory()
        thread = threading.Thread(target=worker.run, args=[queue, lock])
        thread.daemon = True
        thread.start()
    queue.join()

class Command(object):
    def run(self, args):
        raise NotImplementedError('Override me')

class DownloadCommand(Command):
    @staticmethod
    def decorate_parser(parent_parser, fmt):
        for alias in ['download', 'dl']:
            parser = parent_parser.add_parser(
                alias, help='download files', formatter_class=fmt)

            parser.add_argument(
                '-a', '--accept', metavar='REGEX', default='.*',
                help='set regex indicating which URLs to crawl')

            parser.add_argument(
                '-H', '--history', metavar='FILE',
                help='set path to the history file')

            parser.add_argument(
                '-t', '--target', metavar='DIR', default='.',
                help='set base target directory')

            parser.add_argument(
                '-r', '--retries', metavar='NUM', type=int, default=3,
                help='set retry count for failed downloads')

            parser.add_argument(
                '--retry_wait', metavar='NUM', type=int, default=3,
                help='set retry count for failed downloads')

            parser.add_argument(
                '-n', '--num', dest='workers', metavar='NUMBER', type=int,
                default=1, help='set worker count')

            parser.add_argument(
                'url', metavar='URL', nargs='+',
                help='initial URLs to retrieve')

            parser.set_defaults(command=__class__)

    def run(self, args):
        args.accept = re.compile(args.accept)
        visited = set()
        downloaded = read_history_file(args.history)
        queue = Queue()
        for url in args.url:
            visited.add(url)
            queue.put(DownloadCommand.Item(url))
        try:
            f = lambda: __class__.Worker(args, visited, downloaded)
            run_workers(args.workers, queue, f)
        finally:
            save_history_file(downloaded, args.history)

    class Worker(Worker):
        def __init__(self, args, visited, downloaded):
            self.args = args
            self.visited = visited
            self.downloaded = downloaded

        def process_html(self, response, queue):
            soup = BeautifulSoup(response.text)
            child_urls = set()
            for link in soup.find_all('a', href=True):
                child_url = urllib.parse.urldefrag(
                    urllib.parse.urljoin(response.url, link['href'])).url
                if self.args.accept.search(child_url) \
                and not child_url in self.visited \
                and not child_url in self.downloaded:
                    child_urls.add(child_url)
                    self.visited.add(child_url)
            for child_url in sorted(child_urls):
                queue.put(DownloadCommand.Item(child_url))
                self.visited.add(child_url)
            return __class__.Status(parsed=True, urls_added=len(child_urls))

        def process_nonhtml(self, response):
            parsed_url = urllib.parse.urlparse(response.url)
            target_path = os.path.join(
                self.args.target,
                parsed_url.netloc,
                re.sub(r'^[\/]*', '', parsed_url.path))
            target_dir = os.path.dirname(target_path)
            os.makedirs(target_dir, exist_ok=True)
            with open(target_path, 'wb') as fh:
                fh.write(response.content)
            self.downloaded.add(response.url)
            return __class__.Status(download_path=target_path)

        def process(self, item, queue, lock):
            response = requests.get(item.url)
            with lock:
                if response.status_code != 200:
                    raise RuntimeError('HTTP error %d' % response.status_code)
                mime = response.headers['content-type'].split(';')[0].lower()
                if mime == 'text/html':
                    return self.process_html(response, queue)
                else:
                    return self.process_nonhtml(response)

        def on_error(self, item, ex, queue, lock):
            time.sleep(self.args.retry_wait)
            item.retries += 1
            if item.retries < self.args.retries:
                queue.put(item)
            return __class__.Status(error=str(ex))

        def format_status(self, item, status):
            messages = []
            if status.error:
                messages.append('error: %s' % status.error)
            if status.download_path:
                messages.append('saved to %s' % status.download_path)
            if status.parsed:
                messages.append('added %d URLs' % status.urls_added)
            if item.retries > 1:
                messages.append('retry #%d' % item.retries)
            return '%s: %s' % (item.url, '; '.join(messages))

        class Status(object):
            def __init__(
            self, parsed=False, download_path=False, urls_added=0, error=None):
                self.parsed = parsed
                self.download_path = download_path
                self.urls_added = urls_added
                self.error = error

    class Item(object):
        def __init__(self, url):
            self.url = url
            self.retries = 0

class PruneCommand(Command):
    @staticmethod
    def decorate_parser(parent_parser, fmt):
        parser = parent_parser.add_parser(
            'prune', help='prune old entries in history file',
            formatter_class=fmt)

        parser.add_argument(
            '-H', '--history', metavar='FILE',
            help='set path to the history file')

        parser.add_argument(
            '-n', '--num', dest='workers', metavar='NUMBER', type=int,
            default=1, help='set worker count')

        parser.set_defaults(command=__class__)

    def run(self, args):
        downloaded = read_history_file(args.history)
        queue = Queue()
        for url in downloaded:
            queue.put(url)
        try:
            f = lambda: __class__.Worker(downloaded)
            run_workers(args.workers, queue, f)
        finally:
            save_history_file(downloaded, args.history)

    class Worker(Worker):
        def __init__(self, downloaded):
            self.downloaded = downloaded

        def process(self, item, queue, lock):
            response = requests.head(item.strip())
            with lock:
                if response.status_code == 404:
                    self.downloaded.remove(item)
                    return 'pruned (404)'
                else:
                    return 'spared (%d)' % response.status_code

        def on_error(self, item, ex, queue, lock):
            return 'error: %s' % str(ex)

        def format_status(self, item, status):
            return '%s: %s' % (item.strip(), status)

def parse_args():
    fmt = lambda prog: argparse.HelpFormatter(
        prog, max_help_position=40, width=80)

    parser = argparse.ArgumentParser(
        description='Download images from the Internet', formatter_class=fmt)
    subparsers = parser.add_subparsers(help='choose the command')
    DownloadCommand.decorate_parser(subparsers, fmt)
    PruneCommand.decorate_parser(subparsers, fmt)
    return parser.parse_args()

def main():
    args = parse_args()
    command = args.command()
    command.run(args)

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print('Exiting due to user abort')
