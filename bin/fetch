#!/usr/bin/python3
from queue import Queue, LifoQueue
import argparse
import collections
import os
import pickle
import re
import sys
import threading
import time
import urllib.parse
import signal
import sys

stop = False
def set_global_stop():
    global stop
    stop = True
def global_stop():
    return stop

try:
    import lockfile
    import requests
    from bs4 import BeautifulSoup
except ImportError as e:
    if e.name is None:
        raise
    print('Please install %s.' % e.name, file=sys.stderr)
    sys.exit(1)


class PseudoDict(collections.defaultdict):
    __getattr__ = dict.__getitem__
    __setattr__ = dict.__setitem__
    def __init__(self, *args, **kwargs):
        super().__init__(lambda: None)
        self.update(*args, **kwargs)


class History(set):
    def __init__(self, path):
        self.path = path

    def __enter__(self):
        self.fh = None
        if self.path and os.path.exists(self.path):
            self.lock = lockfile.LockFile(self.path)
            self.lock.acquire(timeout=-1)
            self.fh = open(self.path, 'r+b')
            self.update(pickle.load(self.fh))
            print('Loaded %d URLs from history file' % len(self))
        return self

    def __exit__(self, *args):
        if self.fh:
            self.fh.truncate(0)
            self.fh.seek(0)
            s = self.copy()
            pickle.dump(s, self.fh)
            self.fh.close()
            self.lock.release()
            print('Saved %d URLs to history file' % len(s))


class Worker(object):
    num = 0
    def __init__(self):
        self.num = self.__class__.num
        self.__class__.num += 1


class QueueWorker(Worker):
    def __init__(self, queue, lock):
        super().__init__()
        self.queue = queue
        self.lock = lock

    def run(self):
        while not global_stop():
            item = self.queue.get()
            try:
                status = self.process(item)
            except Exception as ex:
                status = self.on_error(item, ex)
            finally:
                self.queue.task_done()
            self.do_print(self.format_status(item, status))

    def do_print(self, text):
        with self.lock:
            print('[T%d ~%5d queued] %s' % (self.num, self.queue.qsize(), text))


def run_workers(queue, count, worker_factory):
    for i in range(count):
        worker = worker_factory()
        thread = threading.Thread(target=worker.run)
        thread.daemon = True
        thread.start()
    while not global_stop() and queue.unfinished_tasks:
        time.sleep(1)


class Command(object):
    pass


class DownloadCommand(Command):
    @staticmethod
    def decorate_parser(parent_parser, fmt):
        for alias in ['download', 'dl']:
            parser = parent_parser.add_parser(
                alias, help='download files', formatter_class=fmt)

            parser.add_argument(
                '-a', '--accept', metavar='REGEX', default='.*',
                help='set regex indicating which URLs to crawl')

            parser.add_argument(
                '-H', '--history', metavar='FILE',
                help='set path to the history file')

            parser.add_argument(
                '-t', '--target', metavar='DIR', default='.',
                help='set base target directory')

            parser.add_argument(
                '-r', '--retries', metavar='NUM', type=int, default=3,
                help='set retry count for failed downloads')

            parser.add_argument(
                '--retry_wait', metavar='NUM', type=int, default=3,
                help='set retry count for failed downloads')

            parser.add_argument(
                '-n', '--num', dest='workers', metavar='NUMBER', type=int,
                default=1, help='set worker count')

            parser.add_argument(
                'url', metavar='URL', nargs='+',
                help='initial URLs to retrieve')

            parser.set_defaults(command=__class__)

    def run(self, args):
        args.accept = re.compile(args.accept)
        visited = set()
        with History(args.history) as history:
            queue = LifoQueue()
            for url in args.url:
                visited.add(url)
                queue.put(DownloadCommand.Item(url))
            lock = threading.Lock()
            run_workers(
                queue,
                args.workers,
                lambda: __class__.Worker(queue, lock, args, visited, history))

    class Worker(QueueWorker):
        def __init__(self, queue, lock, args, visited, history):
            super().__init__(queue, lock)
            self.args = args
            self.visited = visited
            self.history = history

        def process(self, item):
            response = requests.get(item.url, timeout=(3, 3))
            if response.status_code != 200:
                raise RuntimeError('HTTP error %d' % response.status_code)
            mime = response.headers['content-type'].split(';')[0].lower()
            if mime == 'text/html':
                return self._process_html(response)
            else:
                return self._process_nonhtml(response)

        def _process_html(self, response):
            soup = BeautifulSoup(response.text)
            child_urls = set()
            for link in soup.find_all('a', href=True):
                child_url = urllib.parse.urldefrag(
                    urllib.parse.urljoin(response.url, link['href'])).url
                with self.lock:
                    if self.args.accept.search(child_url) \
                    and not child_url in self.visited \
                    and not child_url in self.history:
                        child_urls.add(child_url)
                        self.visited.add(child_url)
            with self.lock:
                for child_url in sorted(child_urls):
                    self.queue.put(DownloadCommand.Item(child_url))
                    self.visited.add(child_url)
            return PseudoDict(parsed=True, urls_added=len(child_urls))

        def _process_nonhtml(self, response):
            parsed_url = urllib.parse.urlparse(response.url)
            target_path = os.path.join(
                self.args.target,
                parsed_url.netloc,
                re.sub(r'^[\/]*', '', parsed_url.path))
            target_dir = os.path.dirname(target_path)
            os.makedirs(target_dir, exist_ok=True)
            with open(target_path, 'wb') as fh:
                fh.write(response.content)
            with self.lock:
                self.history.add(response.url)
            return PseudoDict(download_path=target_path)

        def on_error(self, item, ex):
            time.sleep(self.args.retry_wait)
            item.retries += 1
            if item.retries < self.args.retries:
                self.queue.put(item)
            return PseudoDict(error=str(ex))

        def format_status(self, item, status):
            messages = []
            if status.error:
                messages.append('error: %s' % status.error)
            if status.download_path:
                messages.append('saved to %s' % status.download_path)
            if status.parsed:
                messages.append('added %d URLs' % status.urls_added)
            if item.retries > 1:
                messages.append('retry #%d' % item.retries)
            return '%s: %s' % (item.url, '; '.join(messages))

    class Item(PseudoDict):
        def __init__(self, url):
            super().__init__(url=url, retries=0)


class PruneCommand(Command):
    @staticmethod
    def decorate_parser(parent_parser, fmt):
        parser = parent_parser.add_parser(
            'prune', help='prune old entries in history file',
            formatter_class=fmt)

        parser.add_argument(
            '-H', '--history', metavar='FILE',
            help='set path to the history file')

        parser.add_argument(
            '-n', '--num', dest='workers', metavar='NUMBER', type=int,
            default=1, help='set worker count')

        parser.set_defaults(command=__class__)

    def run(self, args):
        with History(args.history) as history:
            queue = Queue()
            for url in history:
                queue.put(url)
            lock = threading.Lock()
            run_workers(
                queue,
                args.workers,
                lambda: __class__.Worker(queue, lock, history))

    class Worker(QueueWorker):
        def __init__(self, queue, lock, history):
            super().__init__(queue, lock)
            self.history = history

        def process(self, item):
            response = requests.head(item.strip())
            if response.status_code == 404:
                with self.lock:
                    self.history.remove(item)
                return 'pruned (404)'
            return 'spared (%d)' % response.status_code

        def on_error(self, item, ex):
            return 'error: %s' % str(ex)

        def format_status(self, item, status):
            return '%s: %s' % (item.strip(), status)


def parse_args():
    fmt = lambda prog: argparse.HelpFormatter(
        prog, max_help_position=40, width=80)

    parser = argparse.ArgumentParser(
        description='Download images from the Internet', formatter_class=fmt)
    subparsers = parser.add_subparsers(help='choose the command')
    DownloadCommand.decorate_parser(subparsers, fmt)
    PruneCommand.decorate_parser(subparsers, fmt)
    return parser.parse_args()

def main():
    args = parse_args()

    def signal_handler(signal, frame):
        if not global_stop():
            sys.stdout.write('Exiting due to user abort\n')
            set_global_stop()
    signal.signal(signal.SIGINT, signal_handler)

    command = args.command()
    command.run(args)

if __name__ == '__main__':
    main()
